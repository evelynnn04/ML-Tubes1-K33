{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_neurons, init='zero', activation='linear', init_params=None, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Initialize a neural network layer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_neurons : int\n",
    "            Number of neurons in the layer\n",
    "        init : str, optional (default='zero')\n",
    "            Initialization method. Options:\n",
    "            - 'zero': Zero initialization\n",
    "            - 'uniform': Uniform random distribution\n",
    "            - 'normal': Normal (Gaussian) random distribution\n",
    "        activation : str, optional (default='linear')\n",
    "            Activation function to use\n",
    "        init_params : dict, optional\n",
    "            Additional parameters for initialization:\n",
    "            - For 'uniform': \n",
    "                * 'lower': lower bound (default: -1)\n",
    "                * 'upper': upper bound (default: 1)\n",
    "                * 'seed': random seed (optional)\n",
    "            - For 'normal':\n",
    "                * 'mean': mean of distribution (default: 0)\n",
    "                * 'variance': variance of distribution (default: 1)\n",
    "                * 'seed': random seed (optional)\n",
    "        \"\"\"\n",
    "        self.n_neurons = n_neurons\n",
    "        self.init = init\n",
    "        self.activation = activation\n",
    "        self.init_params = init_params or {}\n",
    "        \n",
    "        if self.init == 'uniform':\n",
    "            self.init_params.setdefault('lower', -1)\n",
    "            self.init_params.setdefault('upper', 1)\n",
    "        elif self.init == 'normal':\n",
    "            self.init_params.setdefault('mean', 0)\n",
    "            self.init_params.setdefault('variance', 1)\n",
    "        \n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "    \n",
    "    def initialize(self, input_dim):\n",
    "        if 'seed' in self.init_params:\n",
    "            np.random.seed(self.init_params['seed'])\n",
    "\n",
    "        self.biases = np.zeros((1, self.n_neurons))\n",
    "        if self.init == 'zero':\n",
    "            self.weights = np.zeros((input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'uniform':\n",
    "            lower = self.init_params['lower']\n",
    "            upper = self.init_params['upper']\n",
    "            self.weights = np.random.uniform(low=lower, high=upper, size=(input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'normal':\n",
    "            mean = self.init_params['mean']\n",
    "            variance = self.init_params['variance']\n",
    "            self.weights = np.random.normal(loc=mean, scale=np.sqrt(variance), size=(input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'xavier_uniform':\n",
    "            limit = np.sqrt(6 / (input_dim + self.n_neurons))\n",
    "            self.weights = np.random.uniform(-limit, limit, (input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'xavier_normal':\n",
    "            std = np.sqrt(2 / (input_dim + self.n_neurons))\n",
    "            self.weights = np.random.normal(0, std, (input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'he_normal':\n",
    "            std = np.sqrt(2 / input_dim)\n",
    "            self.weights = np.random.normal(0, std, (input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'he_uniform':\n",
    "            limit = np.sqrt(6 / input_dim)\n",
    "            self.weights = np.random.uniform(-limit, limit, (input_dim, self.n_neurons))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown initialization type: {self.init}\\n\"\n",
    "                \"Available types: zero, uniform, normal, xavier_uniform, xavier_normal, he_normal, he_uniform\"\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def activate(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == 'leaky_relu':\n",
    "            return np.where(x > 0, x, 0.01 * x)\n",
    "        elif self.activation == 'elu':\n",
    "            return np.where(x > 0, x, 0.01 * (np.exp(x) - 1))\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation == 'softmax':\n",
    "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "            return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown activation function: {self.activation}\\n\"\n",
    "                \"/n Activation function available: linear, relu,sigmoid, tanh, softmax, elu, leaky_relu\"\n",
    "            )\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return np.ones_like(x)\n",
    "        elif self.activation == 'relu':\n",
    "            return (x > 0).astype(float)\n",
    "        elif self.activation == 'leaky_relu':\n",
    "            return np.where(x > 0, 1, 0.01)\n",
    "        elif self.activation == 'elu':\n",
    "            alpha = 0.01\n",
    "            return np.where(x > 0, 1, alpha * np.exp(x))\n",
    "        elif self.activation == 'sigmoid':\n",
    "            s = self.activate(x)\n",
    "            return s * (1 - s)\n",
    "        elif self.activation == 'tanh':\n",
    "            t = np.tanh(x)\n",
    "            return 1 - t**2\n",
    "        elif self.activation == 'softmax':\n",
    "            return 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown activation function: {self.activation}/n\"\n",
    "                \"Activation function available: linear, relu,sigmoid, tanh, softmax, elu, leaky_relu\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note:\n",
    "- if y_true.ndim == 1: y_true = y_true.reshape(-1, 1) -> Kalo array 1D ubah jadi array 2D\n",
    "- if y_true.shape != y_pred.shape: y_true = np.eye(y_pred.shape[1])[y_true.flatten()] -> handle kalo y_true contain class label bukan one hot \n",
    "'''\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "def bce(y_true, y_pred):\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    \n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def bce_derivative(y_true, y_pred):\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    \n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "def cce(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    if y_true.ndim == 1 or (y_true.ndim == 2 and y_true.shape[1] == 1):\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    \n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def cce_derivative(y_true, y_pred):\n",
    "    if y_true.ndim == 1 or (y_true.ndim == 2 and y_true.shape[1] == 1):\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    \n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, loss='mse', batch_size=32, learning_rate=0.01, epochs=100, verbose=1):\n",
    "        self.layers = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        if loss == 'mse':\n",
    "            self.loss_func = mse\n",
    "            self.loss_derivative = mse_derivative\n",
    "        elif loss == 'bce':\n",
    "            self.loss_func = bce\n",
    "            self.loss_derivative = bce_derivative\n",
    "        elif loss == 'cce':\n",
    "            self.loss_func = cce\n",
    "            self.loss_derivative = cce_derivative\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown loss function: {loss}\\n\"\n",
    "                \"Loss function available: mse, bce, cce\"\n",
    "            )\n",
    "\n",
    "    '''\n",
    "    LOSS FUNCTION\n",
    "    Note:\n",
    "    - if y_true.ndim == 1: y_true = y_true.reshape(-1, 1) -> Kalo array 1D ubah jadi array 2D\n",
    "    - if y_true.shape != y_pred.shape: y_true = np.eye(y_pred.shape[1])[y_true.flatten()] -> handle kalo y_true contain class label bukan one hot \n",
    "    '''\n",
    "\n",
    "    def mse(y_true, y_pred):\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "        if y_true.shape != y_pred.shape:\n",
    "            y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def mse_derivative(y_true, y_pred):\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "        if y_true.shape != y_pred.shape:\n",
    "            y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "    def bce(y_true, y_pred):\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "        if y_true.shape != y_pred.shape:\n",
    "            y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "        \n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def bce_derivative(y_true, y_pred):\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "        if y_true.shape != y_pred.shape:\n",
    "            y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "        \n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "    def cce(y_true, y_pred):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        if y_true.ndim == 1 or (y_true.ndim == 2 and y_true.shape[1] == 1):\n",
    "            y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "        \n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "    def cce_derivative(y_true, y_pred):\n",
    "        if y_true.ndim == 1 or (y_true.ndim == 2 and y_true.shape[1] == 1):\n",
    "            y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "        \n",
    "        return y_pred - y_true\n",
    "    \n",
    "    def build_layers(self, *layer_args):\n",
    "        self.layers = list(layer_args)\n",
    "    \n",
    "    def _initialize_network(self, input_dim):\n",
    "        prev_dim = input_dim\n",
    "        for layer in self.layers:\n",
    "            layer.initialize(prev_dim)\n",
    "            prev_dim = layer.n_neurons\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            z = activations[-1] @ layer.weights + layer.biases\n",
    "            a = layer.activate(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "        \n",
    "        return zs, activations\n",
    "    \n",
    "    def backward(self, X, y, zs, activations):\n",
    "        m = X.shape[0]\n",
    "        y_pred = activations[-1]\n",
    "        \n",
    "        delta = self.loss_derivative(y, y_pred)\n",
    "        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            z = zs[i]\n",
    "            a_prev = activations[i]\n",
    "            \n",
    "            grad_w = (a_prev.T @ delta) / m\n",
    "            grad_b = np.sum(delta, axis=0, keepdims=True) / m\n",
    "\n",
    "            self.layers[i].weights -= self.learning_rate * grad_w\n",
    "            self.layers[i].biases -= self.learning_rate * grad_b\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = (delta @ self.layers[i].weights.T) * self.layers[i - 1].activation_derivative(zs[i - 1])\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self._initialize_network(X.shape[1])\n",
    "        \n",
    "        # Kalo gaada val data -> trainnya displit \n",
    "        if X_val is None or y_val is None:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        else:\n",
    "            X_train, y_train = X, y\n",
    "        \n",
    "        if self.verbose == 0:\n",
    "            for epoch in range(self.epochs):\n",
    "                indices = np.arange(X.shape[0])\n",
    "                np.random.shuffle(indices)\n",
    "                \n",
    "                for start in range(0, X.shape[0], self.batch_size):\n",
    "                    end = start + self.batch_size\n",
    "                    batch_indices = indices[start:end]\n",
    "                    X_batch = X[batch_indices]\n",
    "                    y_batch = y[batch_indices]\n",
    "                    \n",
    "                    zs, activations = self.forward(X_batch)\n",
    "                    self.backward(X_batch, y_batch, zs, activations)\n",
    "        \n",
    "        elif self.verbose == 1:\n",
    "            for epoch in range(self.epochs):\n",
    "                epoch_progress = tqdm(total=X.shape[0], desc=f\"Epoch {epoch+1}/{self.epochs}\", unit='sample')\n",
    "                \n",
    "                indices = np.arange(X.shape[0])\n",
    "                np.random.shuffle(indices)\n",
    "                \n",
    "                for start in range(0, X.shape[0], self.batch_size):\n",
    "                    end = start + self.batch_size\n",
    "                    batch_indices = indices[start:end]\n",
    "                    X_batch = X[batch_indices]\n",
    "                    y_batch = y[batch_indices]\n",
    "                    \n",
    "                    zs, activations = self.forward(X_batch)\n",
    "                    self.backward(X_batch, y_batch, zs, activations)\n",
    "\n",
    "                    epoch_progress.update(len(X_batch))\n",
    "                \n",
    "                epoch_progress.close()\n",
    "                \n",
    "                y_train_pred = self.forward(X)[1][-1]\n",
    "                train_loss = self.loss_func(y, y_train_pred)\n",
    "                \n",
    "                y_val_pred = self.forward(X_val)[1][-1]\n",
    "                val_loss = self.loss_func(y_val, y_val_pred)\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{self.epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid verbose value: {self.verbose}\\n\"\n",
    "                \"Verbose options: 0 (no output), 1 (progress bar)\"\n",
    "            )\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, activations = self.forward(X)\n",
    "        if self.loss == 'cce':  \n",
    "            return np.argmax(activations[-1], axis=1)\n",
    "        return np.round(activations[-1])\n",
    "    \n",
    "    def save(self, filename):\n",
    "        model_state = {\n",
    "            'layers': self.layers,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'loss': self.loss,\n",
    "            'batch_size': self.batch_size,\n",
    "            'epochs': self.epochs,\n",
    "            'verbose': self.verbose\n",
    "        }\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(model_state, f)\n",
    "\n",
    "        print(f\"Model saved to {filename}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            model_state = pickle.load(f)\n",
    "        \n",
    "        model = cls(\n",
    "            loss=model_state['loss'],\n",
    "            batch_size=model_state['batch_size'],\n",
    "            learning_rate=model_state['learning_rate'],\n",
    "            epochs=model_state['epochs'],\n",
    "            verbose=model_state['verbose']\n",
    "        )\n",
    "        \n",
    "        model.layers = model_state['layers']\n",
    "        print(f\"Model loaded from {filename}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, train_size=1000, test_size=10, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, train_size=0.8, stratify=y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn = FFNN(\n",
    "    loss='cce',\n",
    "    batch_size=1,\n",
    "    learning_rate=0.1,\n",
    "    epochs=20,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn.build_layers(\n",
    "    Layer(n_neurons=2, init='zero', activation='linear'),\n",
    "    Layer(n_neurons=2, init='uniform', activation='relu', init_params={'lower': -0.5, 'upper': 0.5}),\n",
    "    Layer(n_neurons=2, init='normal', activation='tanh', init_params={'mean': 0, 'variance': 0.01}),\n",
    "    Layer(n_neurons=2, init='xavier_normal', activation='softmax'),\n",
    "    Layer(n_neurons=2, init='xavier_uniform', activation='leaky_relu'),\n",
    "    Layer(n_neurons=2, init='he_normal', activation='elu'),\n",
    "    Layer(n_neurons=10, init='he_uniform', activation='sigmoid')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_13196\\528449400.py:93: RuntimeWarning: overflow encountered in exp\n",
      "  return np.where(x > 0, x, 0.01 * (np.exp(x) - 1))\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_13196\\528449400.py:95: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_13196\\528449400.py:116: RuntimeWarning: overflow encountered in exp\n",
      "  return np.where(x > 0, 1, alpha * np.exp(x))\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_13196\\2141830752.py:121: RuntimeWarning: overflow encountered in matmul\n",
      "  delta = (delta @ self.layers[i].weights.T) * self.layers[i - 1].activation_derivative(zs[i - 1])\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_13196\\2141830752.py:121: RuntimeWarning: invalid value encountered in multiply\n",
      "  delta = (delta @ self.layers[i].weights.T) * self.layers[i - 1].activation_derivative(zs[i - 1])\n"
     ]
    }
   ],
   "source": [
    "model_ffnn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_ffnn.predict(X_test)\n",
    "for h in y_pred:\n",
    "    max_index = np.argmax(h)\n",
    "    print(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to my_model.pkl\n"
     ]
    }
   ],
   "source": [
    "model_ffnn.save('my_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from my_model.pkl\n"
     ]
    }
   ],
   "source": [
    "loaded_model = FFNN.load('my_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_model.predict(X_test)\n",
    "for h in y_pred:\n",
    "    max_index = np.argmax(h)\n",
    "    print(max_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

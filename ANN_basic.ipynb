{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_neurons, init='zero', activation='linear', init_params=None, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Initialize a neural network layer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_neurons : int\n",
    "            Number of neurons in the layer\n",
    "        init : str, optional (default='zero')\n",
    "            Initialization method. Options:\n",
    "            - 'zero': Zero initialization\n",
    "            - 'uniform': Uniform random distribution\n",
    "            - 'normal': Normal (Gaussian) random distribution\n",
    "        activation : str, optional (default='linear')\n",
    "            Activation function to use\n",
    "        init_params : dict, optional\n",
    "            Additional parameters for initialization:\n",
    "            - For 'uniform': \n",
    "                * 'lower': lower bound (default: -1)\n",
    "                * 'upper': upper bound (default: 1)\n",
    "                * 'seed': random seed (optional)\n",
    "            - For 'normal':\n",
    "                * 'mean': mean of distribution (default: 0)\n",
    "                * 'variance': variance of distribution (default: 1)\n",
    "                * 'seed': random seed (optional)\n",
    "        \"\"\"\n",
    "        self.n_neurons = n_neurons\n",
    "        self.init = init\n",
    "        self.activation = activation\n",
    "        self.init_params = init_params or {}\n",
    "        \n",
    "        if self.init == 'uniform':\n",
    "            self.init_params.setdefault('lower', -1)\n",
    "            self.init_params.setdefault('upper', 1)\n",
    "        elif self.init == 'normal':\n",
    "            self.init_params.setdefault('mean', 0)\n",
    "            self.init_params.setdefault('variance', 1)\n",
    "        \n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "    \n",
    "    def initialize(self, input_dim):\n",
    "        if 'seed' in self.init_params:\n",
    "            np.random.seed(self.init_params['seed'])\n",
    "\n",
    "        self.biases = np.zeros((1, self.n_neurons))\n",
    "        if self.init == 'zero':\n",
    "            self.weights = np.zeros((input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'uniform':\n",
    "            lower = self.init_params['lower']\n",
    "            upper = self.init_params['upper']\n",
    "            self.weights = np.random.uniform(low=lower, high=upper, size=(input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'normal':\n",
    "            mean = self.init_params['mean']\n",
    "            variance = self.init_params['variance']\n",
    "            self.weights = np.random.normal(loc=mean, scale=np.sqrt(variance), size=(input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'xavier_uniform':\n",
    "            limit = np.sqrt(6 / (input_dim + self.n_neurons))\n",
    "            self.weights = np.random.uniform(-limit, limit, (input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'xavier_normal':\n",
    "            std = np.sqrt(2 / (input_dim + self.n_neurons))\n",
    "            self.weights = np.random.normal(0, std, (input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'he_normal':\n",
    "            std = np.sqrt(2 / input_dim)\n",
    "            self.weights = np.random.normal(0, std, (input_dim, self.n_neurons))\n",
    "        \n",
    "        elif self.init == 'he_uniform':\n",
    "            limit = np.sqrt(6 / input_dim)\n",
    "            self.weights = np.random.uniform(-limit, limit, (input_dim, self.n_neurons))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown initialization type: {self.init}\\n\"\n",
    "                \"Available types: zero, uniform, normal, xavier_uniform, xavier_normal, he_normal, he_uniform\"\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def activate(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == 'leaky_relu':\n",
    "            return np.where(x > 0, x, 0.01 * x)\n",
    "        elif self.activation == 'elu':\n",
    "            return np.where(x > 0, x, 0.01 * (np.exp(x) - 1))\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation == 'softmax':\n",
    "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "            return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown activation function: {self.activation}\\n\"\n",
    "                \"/n Activation function available: linear, relu,sigmoid, tanh, softmax, elu, leaky_relu\"\n",
    "            )\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return np.ones_like(x)\n",
    "        elif self.activation == 'relu':\n",
    "            return (x > 0).astype(float)\n",
    "        elif self.activation == 'leaky_relu':\n",
    "            return np.where(x > 0, 1, 0.01)\n",
    "        elif self.activation == 'elu':\n",
    "            alpha = 0.01\n",
    "            return np.where(x > 0, 1, alpha * np.exp(x))\n",
    "        elif self.activation == 'sigmoid':\n",
    "            s = self.activate(x)\n",
    "            return s * (1 - s)\n",
    "        elif self.activation == 'tanh':\n",
    "            t = np.tanh(x)\n",
    "            return 1 - t**2\n",
    "        elif self.activation == 'softmax':\n",
    "            return 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown activation function: {self.activation}/n\"\n",
    "                \"Activation function available: linear, relu,sigmoid, tanh, softmax, elu, leaky_relu\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note:\n",
    "- if y_true.ndim == 1: y_true = y_true.reshape(-1, 1) -> Kalo array 1D ubah jadi array 2D\n",
    "- if y_true.shape != y_pred.shape: y_true = np.eye(y_pred.shape[1])[y_true.flatten()] -> handle kalo y_true contain class label bukan one hot \n",
    "'''\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "def bce(y_true, y_pred):\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    \n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def bce_derivative(y_true, y_pred):\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    \n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "def cce(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    if y_true.ndim == 1 or (y_true.ndim == 2 and y_true.shape[1] == 1):\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    \n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def cce_derivative(y_true, y_pred):\n",
    "    if y_true.ndim == 1 or (y_true.ndim == 2 and y_true.shape[1] == 1):\n",
    "        y_true = np.eye(y_pred.shape[1])[y_true.flatten()]\n",
    "    \n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, loss='mse', batch_size=32, learning_rate=0.01, epochs=100, verbose=1):\n",
    "        self.layers = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        if loss == 'mse':\n",
    "            self.loss_func = mse\n",
    "            self.loss_derivative = mse_derivative\n",
    "        elif loss == 'bce':\n",
    "            self.loss_func = bce\n",
    "            self.loss_derivative = bce_derivative\n",
    "        elif loss == 'cce':\n",
    "            self.loss_func = cce\n",
    "            self.loss_derivative = cce_derivative\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown loss function: {loss}\\n\"\n",
    "                \"Loss function available: mse, bce, cce\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def build_layers(self, *layer_args):\n",
    "        \"\"\"allows the method to accept a variable number of positional arguments\"\"\"\n",
    "        self.layers = list(layer_args)\n",
    "    \n",
    "    def _initialize_network(self, input_dim):\n",
    "        prev_dim = input_dim\n",
    "        for layer in self.layers:\n",
    "            layer.initialize(prev_dim)\n",
    "            prev_dim = layer.n_neurons\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            z = activations[-1] @ layer.weights + layer.biases\n",
    "            a = layer.activate(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "        \n",
    "        return zs, activations\n",
    "    \n",
    "    def backward(self, X, y, zs, activations):\n",
    "        m = X.shape[0]\n",
    "        y_pred = activations[-1]\n",
    "        \n",
    "        delta = self.loss_derivative(y, y_pred)\n",
    "        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            z = zs[i]\n",
    "            a_prev = activations[i]\n",
    "            \n",
    "            grad_w = (a_prev.T @ delta) / m\n",
    "            grad_b = np.sum(delta, axis=0, keepdims=True) / m\n",
    "\n",
    "            self.layers[i].weights -= self.learning_rate * grad_w\n",
    "            self.layers[i].biases -= self.learning_rate * grad_b\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = (delta @ self.layers[i].weights.T) * self.layers[i - 1].activation_derivative(zs[i - 1])\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self._initialize_network(X.shape[1])\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start in range(0, X.shape[0], self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                X_batch = X[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "                \n",
    "                zs, activations = self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch, zs, activations)\n",
    "            \n",
    "            if self.verbose and epoch % 10 == 0:\n",
    "                y_pred = self.forward(X)[1][-1]\n",
    "                loss = self.loss_func(y, y_pred)\n",
    "                print(f'Epoch {epoch}: Loss = {loss:.4f}')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, activations = self.forward(X)\n",
    "        if self.loss == 'cce':  \n",
    "            return np.argmax(activations[-1], axis=1)\n",
    "        return np.round(activations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, train_size=20000, test_size=10, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, train_size=0.8, stratify=y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn = FFNN(\n",
    "    loss='mse',\n",
    "    batch_size=1,\n",
    "    learning_rate=0.1,\n",
    "    epochs=100,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn.build_layers(\n",
    "    Layer(n_neurons=4, init='zero', activation='linear'),  \n",
    "    Layer(n_neurons=3, init='zero', activation='linear'),\n",
    "    Layer(n_neurons=2, init='zero', activation='linear'), \n",
    "    Layer(n_neurons=10, init='zero', activation='sigmoid') \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.0901\n",
      "Epoch 10: Loss = 0.0900\n",
      "Epoch 20: Loss = 0.0900\n",
      "Epoch 30: Loss = 0.0901\n",
      "Epoch 40: Loss = 0.0900\n",
      "Epoch 50: Loss = 0.0901\n",
      "Epoch 60: Loss = 0.0900\n",
      "Epoch 70: Loss = 0.0901\n",
      "Epoch 80: Loss = 0.0900\n",
      "Epoch 90: Loss = 0.0900\n"
     ]
    }
   ],
   "source": [
    "model_ffnn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_ffnn.predict(X_test)\n",
    "for h in y_pred:\n",
    "    max_index = np.argmax(h)\n",
    "    print(max_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
